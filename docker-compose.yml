# Database and embedding services for codesearch
# The codesearch CLI runs natively and connects to these containerized services
#
# Usage:
#   Default (NVIDIA GPU auto-detect or CPU fallback):
#     docker compose up -d
#     - Automatically uses NVIDIA GPU if nvidia-container-runtime is available
#     - Falls back to CPU if no GPU (slower but works everywhere)
#
#   AMD ROCm GPU (requires AMD MI200/MI300/RX7900 + ROCm 6.2+):
#     docker compose --profile rocm up -d
#
#   Only Qdrant (no embeddings):
#     docker compose up qdrant
#
# Note: Only one vLLM service can run at a time (both use port 8000)

services:
  qdrant:
    image: qdrant/qdrant:latest
    container_name: codesearch-qdrant
    ports:
      - "127.0.0.1:6333:6333"  # REST API port - localhost only
      - "127.0.0.1:6334:6334"  # gRPC port - localhost only
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__LOG_LEVEL=INFO
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:6333/ || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    restart: unless-stopped

  vllm-embeddings:
    image: vllm/vllm-openai:latest
    container_name: codesearch-vllm
    ports:
      - "127.0.0.1:8000:8000"  # OpenAI-compatible API - localhost only
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN:-}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ipc: host
    command:
      - "--model"
      - "BAAI/bge-code-v1"
      - "--task"
      - "embed"
      - "--gpu-memory-utilization"
      - "0.8"
      - "--dtype"
      - "float16"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s  # Allow time for model loading
    restart: unless-stopped

  vllm-embeddings-rocm:
    image: rocm/vllm:latest
    container_name: codesearch-vllm-rocm
    profiles:
      - rocm
    ports:
      - "127.0.0.1:8000:8000"  # OpenAI-compatible API - localhost only
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN:-}
    devices:
      - /dev/kfd
      - /dev/dri
    group_add:
      - video
    ipc: host
    cap_add:
      - SYS_PTRACE
    security_opt:
      - seccomp=unconfined
    command:
      - "--model"
      - "BAAI/bge-code-v1"
      - "--task"
      - "embed"
      - "--gpu-memory-utilization"
      - "0.8"
      - "--dtype"
      - "float16"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s  # Allow time for model loading
    restart: unless-stopped

volumes:
  qdrant_data:
    driver: local