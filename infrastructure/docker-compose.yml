# Shared Infrastructure for Codesearch Multi-Repository Setup
#
# This provides PostgreSQL, Qdrant, vLLM embeddings, and vLLM reranker services
# that multiple repositories can connect to.
#
# This file is automatically managed by the codesearch CLI.
# It will be created in ~/.codesearch/infrastructure/ on first use.

services:
  postgres:
    image: postgres:18
    container_name: codesearch-postgres
    ports:
      - "127.0.0.1:5432:5432"  # Expose to host for native CLI
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      - POSTGRES_DB=codesearch
      - POSTGRES_USER=codesearch
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-codesearch}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U codesearch -d codesearch"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - codesearch
    restart: unless-stopped

  qdrant:
    image: qdrant/qdrant:latest-unprivileged
    container_name: codesearch-qdrant
    ports:
      - "127.0.0.1:6333:6333"  # REST API
      - "127.0.0.1:6334:6334"  # gRPC
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__LOG_LEVEL=INFO
    healthcheck:
      test: ["CMD-SHELL", "timeout 1 bash -c '</dev/tcp/localhost/6333' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    networks:
      - codesearch
    restart: unless-stopped

  vllm-embeddings:
    image: vllm/vllm-openai:v0.11.0
    container_name: codesearch-vllm
    ports:
      - "127.0.0.1:8000:8000"  # OpenAI-compatible API - localhost only
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN:-}
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:64
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ipc: host
    command:
      - "--model"
      - "BAAI/bge-code-v1"
      - "--revision"
      - "bd67852057c5d7ddcc7b8234d9d6c410117ed851"
      - "--task"
      - "embed"
      - "--dtype"
      - "float32"
      - "--max-model-len"
      - "32768"
      - "--max-num-batched-tokens"
      - "32768"
      - "--max-num-seqs"
      - "256"
      - "--gpu_memory_utilization"
      - "0.5"
      - "--trust-remote-code"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s  # Allow time for model loading
    depends_on:
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_healthy
    networks:
      - codesearch
    restart: unless-stopped

  vllm-reranker:
    image: vllm/vllm-openai:latest
    container_name: codesearch-vllm-reranker
    ports:
      - "127.0.0.1:8001:8000"  # OpenAI-compatible API - localhost only (port 8001 on host)
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN:-}
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:64
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    ipc: host
    command:
      - "--model"
      - "BAAI/bge-reranker-v2-m3"
      - "--revision"
      - "953dc6f6f85a1b2dbfca4c34a2796e7dde08d41e"
      - "--task"
      - "embed"
      - "--max-model-len"
      - "8192"
      - "--gpu_memory_utilization"
      - "0.3"
      - "--trust-remote-code"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - codesearch
    restart: unless-stopped

networks:
  codesearch:
    name: codesearch-network
    driver: bridge

volumes:
  postgres_data:
    driver: local
  qdrant_data:
    driver: local
